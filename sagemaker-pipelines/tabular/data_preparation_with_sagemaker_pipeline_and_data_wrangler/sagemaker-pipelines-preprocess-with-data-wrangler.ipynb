{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b54fd606",
   "metadata": {},
   "source": [
    "# Data Preparation and Feature Engineering with Amazon SageMaker Pipeline and Data Wrangler\n",
    "\n",
    "With the introduction of Amazon SageMaker Data Wrangler, data preparation and feature engineering can be easily achieved from Data Wrangler's visual interface. And with Amazon SageMaker Pipeline, tasks of orchestrating SageMaker jobs and authoring reproducible machine learning pipelines are dramatically simplified. \n",
    "\n",
    "This example will focus on combining the force of SageMaker Data Wrangler and SageMaker Pipeline to simplify the process of data preparation and feature engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef94e3",
   "metadata": {},
   "source": [
    "### Create a Data Wrangler Workflow for Data Preparation\n",
    "\n",
    "First, we will start by creating a workflow in SageMaker Data Wrangler's virtual interface. There is a step-by-step [guide](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler.html) to follow in case you are not already familiar with Data Wrangler. The dataset we will use is the [UCI Machine Learning Abalone Dataset](https://archive.ics.uci.edu/ml/datasets/abalone) [1] and we will use Data Wrangler to prepare the data for training. \n",
    "\n",
    "\n",
    "[1] Dua, D. and Graff, C. (2019). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6378a",
   "metadata": {},
   "source": [
    "#### Import Data\n",
    "\n",
    "![Import Data](img/import.png)\n",
    "Make sure to unselect `First row is header` if your dataset doesn't contain header row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e39ab8",
   "metadata": {},
   "source": [
    "#### Use Custom Transform to add Header Row \n",
    "![Add Header Row](img/header.png) \n",
    "(Here we use PySpark SQL in the example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af2c8e2",
   "metadata": {},
   "source": [
    "#### Impute Numeric Columns\n",
    "![Impute](img/impute_numeric.png) \n",
    "(Repeat this operation for all numeric columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a869f",
   "metadata": {},
   "source": [
    "#### Scale Values\n",
    "![Impute](img/scale_values.png) \n",
    "(Repeat this operation for all numeric columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143f5025",
   "metadata": {},
   "source": [
    "#### Impute Categorical\n",
    "![Impute Ctegorical](img/handle_missing.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f6ac5f",
   "metadata": {},
   "source": [
    "#### One-hot Encoding\n",
    "![One-hot Encoding](img/one_hot_encode.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b63b6e",
   "metadata": {},
   "source": [
    "### Export Data Wrangler Workflow\n",
    "\n",
    "Next, we will export the workflow created in the previous step. There are several ways to export a workflow; Data Wrangler offers export options to SageMaker Data Wrangler Job, Pipeline, Python code and Feature Store.\n",
    "\n",
    "The following options create a Jupyter Notebook to execute your data flow and integrate with the respective SageMaker feature.\n",
    "* Data Wrangler Job: launches a SageMaker procesing job to execute your workflow\n",
    "* Pipeline: creates a single step pipeline to execute your workflow\n",
    "* Feature Store: injects outputs your workflow to a SageMaker feature store\n",
    "\n",
    "For a one time data preparation, export the workflow and run the generated notebook. To reuse the workflow in any other pipeline, we will the need the below information from the notebook:\n",
    "* Output Name\n",
    "* S3 URI which the SageMaker Data Wrangler recipe is uploaded to\n",
    "\n",
    "Output Name and S3 URI can be found in the `Parameterers` section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b43252",
   "metadata": {},
   "source": [
    "#### Select Export Steps\n",
    "![Select Steps](img/select_steps.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6fce19",
   "metadata": {},
   "source": [
    "#### Select Export Options\n",
    "![Select Export Option](img/export.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353559c2",
   "metadata": {},
   "source": [
    "#### Take a Note of Output Name\n",
    "![Output Name](img/output_name.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8604c",
   "metadata": {},
   "source": [
    "#### Take a Note of S3 URI of Workflow Recipe \n",
    "![flow](img/upload_flow_to_s3.png) \n",
    "(Sample from pipeline notebook, execute the cell will upload Data Wrangler workflow recipe to S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292d657e",
   "metadata": {},
   "source": [
    "### Run Data Wrangler Workflow in SageMaker Pipeline\n",
    "\n",
    "Next, we will demonstrate how to reuse Data Wrangler workflow in any SageMaker Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6d1dab",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737bc6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.wrangler.processing import DataWranglerProcessor\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "# Parameters\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker\n",
    "base_dir = \"/opt/ml/processing\"\n",
    "\n",
    "abalone_sample_data = \"s3://my-sample-data/abalone.csv\"\n",
    "output_content_type = \"CSV\"\n",
    "output_name = \"0a562d05-69dc-4bc0-ab95-9dd0d2e4b6a3.default\"\n",
    "\n",
    "data_wrangler_recipe_s3_uri = \"s3://my-data-wrangler-workflow/sample.flow\"\n",
    "data_wrangler_instance_count = ParameterInteger(name=\"InstanceCount\", default_value=1)\n",
    "data_wrangler_instance_type = ParameterString(name=\"InstanceType\", default_value=\"ml.m5.4xlarge\")\n",
    "output_config = {output_name: {\"content_type\": output_content_type}}\n",
    "job_argument = [f\"--output-config '{json.dumps(output_config)}'\"]\n",
    "output_s3_uri = f\"s3://{sagemaker_session.default_bucket()}/output\"\n",
    "\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "model_path = f\"s3://{sagemaker_session.default_bucket()}/AbaloneTrain\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5be28",
   "metadata": {},
   "source": [
    "##### Create Data Wrangler Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b2f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangler Job\n",
    "inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name=\"abalone.csv\",\n",
    "        source=abalone_sample_data,\n",
    "        destination=f\"{base_dir}/abalone.csv\",\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        s3_input_mode=\"File\",\n",
    "        s3_data_distribution_type=\"FullyReplicated\",\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "outputs = [\n",
    "    ProcessingOutput(\n",
    "        output_name=output_name,\n",
    "        source=f\"{base_dir}/output\",\n",
    "        destination=output_s3_uri,\n",
    "        s3_upload_mode=\"EndOfJob\",\n",
    "    )\n",
    "]\n",
    "\n",
    "data_wrangler_processor = DataWranglerProcessor(\n",
    "    role=role,\n",
    "    data_wrangler_flow_source=data_wrangler_recipe_s3_uri,\n",
    "    instance_count=data_wrangler_instance_count,\n",
    "    instance_type=data_wrangler_instance_type,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    max_runtime_in_seconds=86400,\n",
    ")\n",
    "\n",
    "data_wrangler_step = ProcessingStep(\n",
    "    name=\"data-wrangler-step\",\n",
    "    processor=data_wrangler_processor,\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    job_arguments=job_argument,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c36968",
   "metadata": {},
   "source": [
    "##### Create Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc0365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Job\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    output_path=model_path,\n",
    "    role=role,\n",
    ")\n",
    "xgb_train.set_hyperparameters(\n",
    "    objective=\"reg:linear\",\n",
    "    num_round=50,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.7,\n",
    "    silent=0,\n",
    ")\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name=\"abalone-training-step\",\n",
    "    estimator=xgb_train,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=data_wrangler_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                output_name\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b157c774",
   "metadata": {},
   "source": [
    "##### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=\"sample-pipeline\",\n",
    "    parameters=[data_warngler_instance_count, data_wrangler_instance_type, training_instance_type],\n",
    "    steps=[data_wrangler_step, training_step],\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb0976c",
   "metadata": {},
   "source": [
    "##### (Optional) Examining the pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a366b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e12267",
   "metadata": {},
   "source": [
    "##### Submit the pipeline to SageMaker and start execution\n",
    "\n",
    "Submit the pipeline definition to the Pipeline service. The role passed in will be used by the Pipeline service to create all the jobs defined in the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ba0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbee81b5",
   "metadata": {},
   "source": [
    "Start the pipeline and accept all the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccc248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
